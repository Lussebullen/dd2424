{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "N = 10000   # Number of data points\n",
    "K = 10      # Number of classes\n",
    "D = 3072    # Number of dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "    def __init__(self, filename=None):\n",
    "\n",
    "        self.data = None\n",
    "        self.hot = None\n",
    "        self.labels = None\n",
    "\n",
    "        if filename:\n",
    "            self.setDataFromFile(filename)\n",
    "            self.transform()\n",
    "\n",
    "        # Precompute flipped indices\n",
    "        indices = np.array(range(D))\n",
    "        imageidx = indices.reshape(32, 32, 3, order=\"F\")\n",
    "        flipped_imageidx = np.flip(imageidx, axis=0)\n",
    "        self.flipped_idx = flipped_imageidx.reshape(D, order=\"F\")\n",
    "\n",
    "    def loadData(self, filename):\n",
    "        \"\"\" Copied from the dataset website \"\"\"\n",
    "        import pickle\n",
    "        with open('./Datasets/cifar-10-batches-py/'+filename, 'rb') as fo:\n",
    "            batch = pickle.load(fo, encoding='bytes')\n",
    "        \n",
    "        labels = np.array(batch[b\"labels\"])\n",
    "\n",
    "        dict = {\n",
    "         \"labels\": labels,\n",
    "         \"data\": batch[b\"data\"].T.astype(float),\n",
    "         \"hot\": self.onehotencoding(labels)\n",
    "         }\n",
    "\n",
    "        return dict\n",
    "\n",
    "    def onehotencoding(self, labels):\n",
    "        \"\"\"\n",
    "        One-hot encodes the given labels.\n",
    "        \n",
    "        Args:\n",
    "            labels (np.array): The labels to be one-hot encoded.\n",
    "            \n",
    "        Returns:\n",
    "            np.array: The one-hot encoded labels.\n",
    "        \"\"\"\n",
    "        N = len(labels)\n",
    "        hot = np.zeros((K, N))\n",
    "        for i in range(N):\n",
    "            hot[labels[i]][i] = 1\n",
    "        return hot\n",
    "\n",
    "    def setDataFromFile(self, fname):\n",
    "        \"\"\"\n",
    "        Sets the data to the contents at the file filename.        \n",
    "        \"\"\"\n",
    "        batch = self.loadData(fname)\n",
    "        self.labels = batch[\"labels\"]\n",
    "        self.data = batch[\"data\"]\n",
    "        self.hot = batch[\"hot\"]\n",
    "    \n",
    "    def concatData(self, fname):\n",
    "        \"\"\"\n",
    "        Concatenates the data from the given filename to the existing data.\n",
    "        \n",
    "        Args:\n",
    "            fname (str): The path to the file containing the data.\n",
    "        \"\"\"\n",
    "        \n",
    "        batch = self.loadData(fname)\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            self.labels = np.concatenate((self.labels, batch[\"labels\"]))\n",
    "        else:\n",
    "            self.labels = batch[\"labels\"]\n",
    "        \n",
    "        if self.data is not None:\n",
    "            self.data = np.concatenate((self.data, batch[\"data\"]), axis=1)\n",
    "        else:\n",
    "            self.data = batch[\"data\"]\n",
    "\n",
    "        self.hot = self.onehotencoding(self.labels)\n",
    "\n",
    "    def transform(self):\n",
    "        \"\"\"\n",
    "        Transforms the given dataset by normalizing the data.\n",
    "        \"\"\"\n",
    "        meanX = np.mean(self.data, axis=0)\n",
    "        stdX = np.std(self.data, axis=0)\n",
    "        self.data = (self.data - meanX) / stdX\n",
    "    \n",
    "    def shuffle(self):\n",
    "        \"\"\"\n",
    "        Shuffles the data.\n",
    "        \"\"\"\n",
    "        permutation = np.random.permutation(self.data.shape[1])\n",
    "        self.data = self.data[:, permutation]\n",
    "        self.hot = self.hot[:, permutation]\n",
    "        self.labels = self.labels[permutation]\n",
    "    \n",
    "    def miniBatch(self, batch, range):\n",
    "        \"\"\"\n",
    "        Creates a mini-batch of the given batch.\n",
    "        \"\"\"\n",
    "        self.data = batch.data[:, range[0]:range[1]]\n",
    "        self.hot = batch.hot[:, range[0]:range[1]]\n",
    "        self.labels = batch.labels[range[0]:range[1]]\n",
    "    \n",
    "    def flip(self, pflip):\n",
    "        \"\"\"\n",
    "        Flips the data with a given probability.\n",
    "        \n",
    "        Args:\n",
    "            pflip (float): The probability of flipping the data.\n",
    "        \"\"\"\n",
    "        flip = np.random.rand(self.data.shape[1]) < pflip\n",
    "        self.data[:,flip] = self.data[:,flip][self.flipped_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    def __init__(self, row, col, seed = None):\n",
    "        \"\"\"\n",
    "        Initializes the weights and biases of the layer.\n",
    "        \"\"\"\n",
    "        if seed:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        self.W = np.random.normal(0, 1/np.sqrt(col), size=(row, col))\n",
    "        self.b = np.zeros((row, 1))\n",
    "        self.h = None\n",
    "        self.gradW = None\n",
    "        self.gradB = None\n",
    "    \n",
    "    # Activation functions #\n",
    "    def softmax(self, x):\n",
    "        \"\"\" Standard definition of the softmax function \"\"\"\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\" Standard definition of the sigmoid function \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def relu(self, x):\n",
    "        \"\"\" Standard definition of the ReLU function \"\"\"\n",
    "        return np.max(0, x)\n",
    "    \n",
    "    ########################################\n",
    "\n",
    "    def forward(self, batch, activation=\"softmax\"):\n",
    "        \"\"\"\n",
    "        Evaluate the classifier for a given input.\n",
    "\n",
    "        Args:\n",
    "            data (dict): A dictionary containing the data and one-hot encoded labels.\n",
    "        \"\"\"\n",
    "        X = batch.data\n",
    "        s = self.W @ X + self.b\n",
    "\n",
    "        if activation == \"softmax\":\n",
    "            self.h = self.softmax(s)\n",
    "        else:\n",
    "            self.h = self.sigmoid(s)\n",
    "    \n",
    "    def lcross(self, batch):\n",
    "        \"\"\"\n",
    "        Calculates the cross-entropy loss.\n",
    "\n",
    "        Args:\n",
    "            data (dict): A dictionary containing the data and one-hot encoded labels.\n",
    "\n",
    "        Returns:\n",
    "            float: Cross-entropy loss.\n",
    "        \"\"\"\n",
    "        Y = batch.hot\n",
    "        return - Y * np.log(self.h)\n",
    "    \n",
    "    def lmultiplebce(self, batch):\n",
    "        \"\"\"\n",
    "        Calculates the K-binary cross-entropy loss.\n",
    "\n",
    "        Args:\n",
    "            data (dict): A dictionary containing the data and one-hot encoded labels.\n",
    "\n",
    "        Returns:\n",
    "            float: Binary cross-entropy loss.\n",
    "        \"\"\"\n",
    "        Y = batch.hot\n",
    "        return -Y * np.log(self.h) - (1 - Y) * np.log(1 - self.h)\n",
    "    \n",
    "    def computeCost(self, batch, lmda, loss = \"lcross\"):\n",
    "        \"\"\"\n",
    "        Compute the cost function for linear regression with regularization.\n",
    "\n",
    "        Args:\n",
    "            data (dict): A dictionary containing the data and one-hot encoded labels.\n",
    "            lmda (float): Regularization parameter.\n",
    "\n",
    "        Returns:\n",
    "        float: The computed cost.\n",
    "        \"\"\"\n",
    "        X = batch.data\n",
    "\n",
    "        reg_term = lmda * np.sum(self.W ** 2)\n",
    "\n",
    "        if loss == \"lcross\":\n",
    "            loss_cross = self.lcross(batch)\n",
    "            denom = X.shape[1]\n",
    "        else:\n",
    "            loss_cross = self.lmultiplebce(batch)\n",
    "            denom = K\n",
    "\n",
    "        return 1 / denom * np.sum(loss_cross) + reg_term, np.sum(loss_cross)\n",
    "    \n",
    "    def computeAcc(self, batch):\n",
    "        \"\"\"\n",
    "        Compute the accuracy of the classifier.\n",
    "\n",
    "        Args:\n",
    "            data (dict): A dictionary containing the data and one-hot encoded labels.\n",
    "\n",
    "        Returns:\n",
    "            float: Accuracy of the classifier.\n",
    "\n",
    "        \"\"\"\n",
    "        Y = batch.hot\n",
    "        pred = np.argmax(self.h, axis=0)\n",
    "        return np.mean(pred == np.argmax(Y, axis=0))\n",
    "    \n",
    "    def backward(self, batch, lmbd):\n",
    "        \"\"\"\n",
    "        Compute the gradients of the cost function with respect to the parameters.\n",
    "\n",
    "        Args:\n",
    "            data (dict): A dictionary containing the data and one-hot encoded labels.\n",
    "            lmbd (float): Regularization parameter.\n",
    "\n",
    "        Returns:\n",
    "            list: A list containing the gradients of the cost function with respect to the weight matrix W and the bias vector b.\n",
    "        \"\"\"\n",
    "        X, Y = batch.data, batch.hot\n",
    "    \n",
    "        G = self.h - Y\n",
    "\n",
    "        self.gradB = (1 / X.shape[1] * np.sum(G, axis = 1)).reshape((K,1))\n",
    "        \n",
    "        self.gradW = 1 / X.shape[1] * G @ X.T + 2 * lmbd * self.W\n",
    "\n",
    "    def update(self, eta):\n",
    "            \"\"\"\n",
    "            Update the parameters of the model.\n",
    "            \"\"\"\n",
    "            self.W -= eta * self.gradW\n",
    "            self.b -= eta * self.gradB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visualizer:\n",
    "    def plotResults(self, title, costs, loss, accs, test_acc = None):\n",
    "        \"\"\"\n",
    "        Plot the results of the training.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(16, 6))\n",
    "        plt.suptitle(title)\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(costs[\"train\"], label=\"Training\")\n",
    "        plt.plot(costs[\"val\"], label=\"Validation\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Cost\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(loss[\"train\"], label=\"Training\")\n",
    "        plt.plot(loss[\"val\"], label=\"Validation\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(accs[\"train\"], label=\"Training\")\n",
    "        plt.plot(accs[\"val\"], label=\"Validation\")\n",
    "        if test_acc:\n",
    "            plt.axhline(test_acc, color=\"red\", label=\"Test Accuracy\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()    \n",
    "        plt.show()\n",
    "\n",
    "    def genWeightImage(self, slice):\n",
    "        \"\"\"\n",
    "        Generates an image from the given slice of the weight matrix.\n",
    "        \n",
    "        Args:\n",
    "            slice (numpy.ndarray): The slice of the weight matrix.\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: The generated image.\n",
    "        \"\"\"\n",
    "        \n",
    "        img = slice.reshape(32, 32, 3, order=\"F\")\n",
    "        img = img - np.min(img)\n",
    "        img = img / np.max(img)\n",
    "        return img\n",
    "\n",
    "    def genMatrixVisualization(self, title, W):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        for i in range(K):\n",
    "            plt.subplot(2, 5, i+1)\n",
    "            plt.imshow(self.genWeightImage(W[i, :]))\n",
    "            plt.title(f\"Slice {i}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.suptitle(title)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def miniBatchGD(train, lmbd=0.1, n_batch=100, eta=0.001, n_epochs=20, val=None, pflip=0):\n",
    "        \"\"\"\n",
    "        Perform mini-batch gradient descent.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data of shape (d, N).\n",
    "            Y (numpy.ndarray): One-hot encoded true label of shape (K, N).\n",
    "            W (numpy.ndarray): Weight matrix of shape (K, d).\n",
    "            b (numpy.ndarray): Bias vector of shape (K, 1).\n",
    "            lmbd (float, optional): Regularization parameter. Defaults to 0.1.\n",
    "            n_batch (int, optional): Number of mini-batches. Defaults to 100.\n",
    "            eta (float, optional): Learning rate. Defaults to 0.001.\n",
    "            n_epochs (int, optional): Number of epochs. Defaults to 20.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the weight matrix W and the bias vector b.\n",
    "        \"\"\"\n",
    "        np.random.seed(20240405)\n",
    "        \n",
    "        costs = {\"train\" : [], \"val\" : []}\n",
    "        loss = {\"train\" : [], \"val\" : []}\n",
    "        accs = {\"train\" : [], \"val\" : []}\n",
    "\n",
    "        model = LinearLayer()\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            # Shuffle data\n",
    "            train.shuffle()\n",
    "\n",
    "            n_mini_batches = int(N / n_batch)\n",
    "            for j in range(n_mini_batches):\n",
    "                # Assemble mini-batch\n",
    "                batch = Data()\n",
    "                j_start = j * n_batch\n",
    "                j_end = (j + 1) * n_batch\n",
    "                batch.miniBatch(train, [j_start, j_end])\n",
    "                if pflip > 0:\n",
    "                    batch.flip(pflip)\n",
    "\n",
    "                model.forward(batch)\n",
    "                model.backward(batch, lmbd)\n",
    "                model.update(eta)\n",
    "\n",
    "            c, l = model.computeCost(batch, lmbd)\n",
    "            costs[\"train\"].append(c)\n",
    "            loss[\"train\"].append(l)\n",
    "            accs[\"train\"].append(model.computeAcc(batch))\n",
    "            if val:\n",
    "                model.forward(val)\n",
    "                c, l = model.computeCost(val, lmbd)\n",
    "                costs[\"val\"].append(c)\n",
    "                loss[\"val\"].append(l)\n",
    "                accs[\"val\"].append(model.computeAcc(val))\n",
    "        return model, costs, loss, accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Parameters\n",
    "\n",
    "Restructure for each layer to contain backward pass update depending on activation funcion for modularity.\n",
    "See p. 36 in slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 50\n",
    "\n",
    "W1 = LinearLayer(M, D)\n",
    "W2 = LinearLayer(K, M)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
